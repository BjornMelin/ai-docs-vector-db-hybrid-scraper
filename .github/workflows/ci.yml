name: Continuous Integration

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write
  issues: write

env:
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1
  PIP_DISABLE_PIP_VERSION_CHECK: 1
  UV_SYSTEM_PYTHON: 1
  # Optimization: Enable UV cache compression
  UV_CACHE_COMPRESSION: 1

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      python-code: ${{ steps.changes.outputs.python-code }}
      tests: ${{ steps.changes.outputs.tests }}
      config: ${{ steps.changes.outputs.config }}
      docs: ${{ steps.changes.outputs.docs }}
      dependencies: ${{ steps.changes.outputs.dependencies }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect file changes
        uses: dorny/paths-filter@v3
        id: changes
        with:
          filters: |
            python-code:
              - 'src/**/*.py'
              - 'scripts/**/*.py'
            tests:
              - 'tests/**/*.py'
              - 'tests/**/*.json'
              - 'tests/**/*.yml'
            config:
              - 'pyproject.toml'
              - 'uv.lock'
              - '.github/workflows/**'
              - 'config/**'
            docs:
              - 'docs/**'
              - '*.md'
            dependencies:
              - 'requirements.txt'
              - 'requirements-dev.txt'
              - 'pyproject.toml'
              - 'uv.lock'

  lint-and-format:
    name: Lint and Format
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.python-code == 'true' || needs.detect-changes.outputs.tests == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
          enable-cache: true
          cache-dependency-glob: "**/uv.lock"

      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/uv
            .venv
          key: python-lint-${{ runner.os }}-${{ hashFiles('**/uv.lock', 'pyproject.toml') }}
          restore-keys: |
            python-lint-${{ runner.os }}-

      - name: Install dependencies
        run: |
          uv sync --dev --frozen

      - name: Cache ruff
        uses: actions/cache@v4
        with:
          path: ~/.cache/ruff
          key: ruff-${{ runner.os }}-${{ hashFiles('pyproject.toml') }}

      - name: Run ruff check
        run: |
          uv run ruff check . --output-format=github

      - name: Run ruff format check
        run: |
          uv run ruff format --check .

      - name: Check import sorting
        run: |
          uv run ruff check --select I .

  test:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    needs: detect-changes
    if: needs.detect-changes.outputs.python-code == 'true' || needs.detect-changes.outputs.tests == 'true' || needs.detect-changes.outputs.config == 'true'
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.11', '3.12']
        include:
          - os: ubuntu-latest
            python-version: '3.12'
            coverage: true
          - os: windows-latest
            python-version: '3.12'
            coverage: false
            # Windows-specific test configuration
            test-marker: "not slow and not integration"
          - os: macos-latest
            python-version: '3.12'
            coverage: false
            # macOS-specific test configuration  
            test-marker: "not slow and not integration"
        exclude:
          # Reduce matrix size for faster CI - focus on Python 3.12 for non-Linux
          - os: windows-latest
            python-version: '3.11'
          - os: macos-latest
            python-version: '3.11'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
          enable-cache: true

      - name: Install dependencies
        run: |
          if [ "${{ runner.os }}" == "Windows" ]; then
            # Windows: Install build tools first, then dependencies
            echo "Installing build dependencies for Windows..."
            uv pip install --upgrade pip setuptools wheel hatchling
            
            # Install core dependencies first
            uv sync --dev --frozen || {
              echo "Fallback: Installing with build isolation disabled"
              uv pip install setuptools wheel hatchling
              uv pip install -e . --no-build-isolation
              uv pip install pytest-timeout pytest-xdist --no-build-isolation
              uv sync --dev --frozen --no-build-isolation || echo "Fallback completed"
            }
            # Ensure pytest plugins are available on Windows
            uv pip install pytest-timeout pytest-xdist || echo "pytest plugins already available"
          else
            uv sync --dev --frozen
            # Force install pytest plugins to ensure they are available
            echo "Installing pytest plugins..."
            uv pip install pytest-timeout>=2.3.1 pytest-xdist>=3.0.0
          fi
          
          # Verify pytest-timeout is installed and working
          echo "Verifying pytest-timeout installation..."
          uv run python -c "import pytest_timeout; print('✅ pytest-timeout available')" || echo "❌ pytest-timeout not available"
        env:
          # Cross-platform environment variables for native dependencies
          PYTHONUTF8: 1
          # Windows: Set compiler flags for native extensions
          DISTUTILS_USE_SDK: 1
          MSSdk: 1
        shell: bash

      - name: Setup browser dependencies (if needed)
        shell: bash
        run: |
          # Only install browser dependencies if browser tests exist
          if [ -f "scripts/test_browser_setup.py" ] && grep -q "browser" tests/unit/**/*.py 2>/dev/null; then
            echo "Setting up browsers for browser-specific tests..."
            uv run python scripts/test_browser_setup.py || {
              echo "⚠️ Browser setup failed - browser tests may be skipped"
              export SKIP_BROWSER_TESTS=1
            }
          else
            echo "No browser tests detected, skipping browser setup"
            export SKIP_BROWSER_TESTS=1
          fi

      - name: Setup test environment
        shell: bash
        run: |
          # Create test directories and data fixtures
          mkdir -p tests/fixtures/{cache,data,logs,vectors,embeddings} logs cache data
          
          # Set proper permissions (Unix-like systems)
          if [ "${{ runner.os }}" != "Windows" ]; then
            chmod -R 755 tests logs cache data
          fi
          
          # Set environment variables for testing
          echo "PYTHONIOENCODING=utf-8" >> $GITHUB_ENV
          echo "TESTING=true" >> $GITHUB_ENV
          echo "LOG_LEVEL=INFO" >> $GITHUB_ENV
          echo "SKIP_BROWSER_TESTS=1" >> $GITHUB_ENV
          echo "SKIP_INTEGRATION_TESTS=1" >> $GITHUB_ENV
          
          # Platform-specific environment setup
          if [ "${{ runner.os }}" == "Windows" ]; then
            echo "PYTHONUTF8=1" >> $GITHUB_ENV
          fi

      - name: Run comprehensive test suite
        shell: bash
        run: |
          # Set test markers based on platform and test type
          if [ "${{ runner.os }}" == "Windows" ] || [ "${{ runner.os }}" == "macOS" ]; then
            TEST_MARKERS="not slow and not integration and not gpu"
            MAX_FAILS=5
            TIMEOUT=300
          else
            TEST_MARKERS="not slow and not gpu"
            MAX_FAILS=10  
            TIMEOUT=180
          fi
          
          COVERAGE_ARGS="${{ matrix.coverage && '--cov=src --cov-report=xml --cov-report=html --cov-branch --cov-fail-under=60' || '' }}"
          
          echo "🧪 Running unit tests..."
          uv run pytest tests/unit \
            -m "$TEST_MARKERS" \
            --tb=short \
            --maxfail=$MAX_FAILS \
            --timeout=$TIMEOUT \
            --disable-warnings \
            --durations=10 \
            -v \
            $COVERAGE_ARGS

      - name: Run integration tests
        run: |
          echo "🔗 Running integration tests..."
          if [ "${{ runner.os }}" == "Linux" ]; then
            # Full integration tests on Linux
            uv run pytest tests/integration \
              --tb=short \
              --maxfail=5 \
              --timeout=300 \
              --disable-warnings \
              -v
          else
            # Core integration tests only on Windows/macOS
            uv run pytest tests/integration \
              -m "not slow and not external_api" \
              --tb=short \
              --maxfail=3 \
              --timeout=240 \
              --disable-warnings \
              -v
          fi
        continue-on-error: ${{ runner.os != 'Linux' }}

      - name: Upload coverage reports
        if: matrix.coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload coverage artifact
        if: matrix.coverage
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            htmlcov/
            coverage.xml
          retention-days: 30

      - name: Run API and core functionality tests
        run: |
          echo "🚀 Testing core application functionality..."
          
          # Test core services and APIs
          if [ -d "tests/unit/services" ]; then
            echo "Testing core services..."
            uv run pytest tests/unit/services/ \
              --tb=short \
              --maxfail=5 \
              --disable-warnings \
              -v
          fi
          
          # Test API endpoints if they exist
          if [ -d "tests/unit/api" ] || find tests -name "*api*" -type f | grep -q .; then
            echo "Testing API endpoints..."
            uv run pytest tests/ -k "api" \
              --tb=short \
              --maxfail=3 \
              --disable-warnings \
              -v
          fi
          
          # Test database functionality
          if find tests -name "*database*" -o -name "*db*" -o -name "*vector*" | grep -q .; then
            echo "Testing database and vector functionality..."
            uv run pytest tests/ -k "database or db or vector" \
              --tb=short \
              --maxfail=3 \
              --disable-warnings \
              -v
          fi
        continue-on-error: false

      - name: Check coverage threshold
        if: matrix.coverage
        run: |
          echo "📊 Checking test coverage..."
          uv run coverage report --show-missing
          uv run coverage report --fail-under=60

  type-check:
    name: Type Checking
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.python-code == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
          enable-cache: true

      - name: Install dependencies
        run: |
          uv sync --dev --frozen

      - name: Run mypy (if configured)
        run: |
          if [ -f "pyproject.toml" ] && grep -q "mypy" pyproject.toml; then
            uv run mypy src/
          else
            echo "mypy not configured, skipping type checking"
          fi
        continue-on-error: true

  dependency-check:
    name: Dependency Security
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.dependencies == 'true' || needs.detect-changes.outputs.config == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
          enable-cache: true

      - name: Install dependencies
        run: |
          uv sync --dev --frozen

      - name: Run pip-audit
        run: |
          uv tool install pip-audit
          uv run pip-audit --format=json --output=audit-results.json || true
          if [ -f "audit-results.json" ]; then
            uv run pip-audit --format=table
          fi
        continue-on-error: true

      - name: Upload security scan results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: dependency-security-scan
          path: audit-results.json
          retention-days: 30

  build-test:
    name: Build Test
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.python-code == 'true' || needs.detect-changes.outputs.config == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
          enable-cache: true

      - name: Build package
        run: |
          uv build

      - name: Check package
        run: |
          uv tool install twine
          uv run twine check dist/*

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: dist/
          retention-days: 7

  docker-test:
    name: Docker Build Test
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.python-code == 'true' || needs.detect-changes.outputs.config == 'true'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build worker Docker image
        if: hashFiles('docker/Dockerfile.worker') != ''
        run: |
          docker build -f docker/Dockerfile.worker -t test-worker .

      - name: Test Docker services
        if: hashFiles('docker-compose.yml') != ''
        run: |
          # Test docker compose configuration
          docker compose config

  performance-test:
    name: Performance & Smoke Tests
    runs-on: ubuntu-latest
    needs: [test, lint-and-format]
    if: always() && (needs.test.result == 'success' || needs.test.result == 'skipped')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
          enable-cache: true

      - name: Install dependencies
        run: |
          uv sync --dev --frozen

      - name: Run smoke tests
        run: |
          echo "🔥 Running smoke tests for core functionality..."
          
          # Test basic imports and module loading
          uv run python -c "import src; print('✅ Core module imports successful')"
          
          # Test configuration loading
          if [ -f "src/config/__init__.py" ]; then
            uv run python -c "from src.config import *; print('✅ Configuration loading successful')"
          fi
          
          # Test database connections (without external dependencies)
          if find src -name "*database*" -o -name "*db*" | grep -q .; then
            uv run python -c "print('🔍 Database modules found - testing imports')"
            # Add basic database import tests here
          fi
          
          # Test vector/embedding functionality  
          if find src -name "*vector*" -o -name "*embed*" | grep -q .; then
            uv run python -c "print('🧮 Vector/embedding modules found - testing imports')"
            # Add basic vector import tests here
          fi

      - name: Run performance benchmarks
        run: |
          echo "⚡ Running performance benchmarks..."
          
          # Run performance tests if they exist
          if [ -d "tests/performance" ]; then
            uv run pytest tests/performance -m "not slow" --tb=short -v
          elif [ -f "scripts/benchmark_query_api.py" ]; then
            timeout 300 uv run python scripts/benchmark_query_api.py --quick || echo "Benchmark completed with timeout"
          elif find scripts -name "*benchmark*" -o -name "*performance*" | head -1 | grep -q .; then
            BENCH_SCRIPT=$(find scripts -name "*benchmark*" -o -name "*performance*" | head -1)
            echo "Running benchmark script: $BENCH_SCRIPT"
            timeout 300 uv run python "$BENCH_SCRIPT" || echo "Benchmark completed"
          else
            echo "No specific performance tests found - running general performance checks"
            # Run a subset of tests with timing
            uv run pytest tests/unit -k "not slow" --durations=20 --tb=no -q
          fi
        continue-on-error: true

  collect-results:
    name: Collect Results
    runs-on: ubuntu-latest
    needs: [lint-and-format, test, type-check, dependency-check, build-test, docker-test, performance-test]
    if: always()
    
    steps:
      - name: Check job results
        run: |
          echo "📋 CI Pipeline Results Summary:"
          echo "================================"
          echo "Lint and Format: ${{ needs.lint-and-format.result }}"
          echo "Test Suite: ${{ needs.test.result }}"
          echo "Type Check: ${{ needs.type-check.result }}"
          echo "Dependency Check: ${{ needs.dependency-check.result }}"
          echo "Build Test: ${{ needs.build-test.result }}"
          echo "Docker Test: ${{ needs.docker-test.result }}"
          echo "================================"
          
          # Check critical job results
          CRITICAL_FAILURES=0
          
          if [[ "${{ needs.lint-and-format.result }}" == "failure" ]]; then
            echo "❌ CRITICAL: Lint and Format checks failed"
            CRITICAL_FAILURES=$((CRITICAL_FAILURES + 1))
          fi
          
          if [[ "${{ needs.test.result }}" == "failure" ]]; then
            echo "❌ CRITICAL: Test suite failed"
            CRITICAL_FAILURES=$((CRITICAL_FAILURES + 1))
          fi
          
          if [[ "${{ needs.build-test.result }}" == "failure" ]]; then
            echo "❌ CRITICAL: Build test failed"
            CRITICAL_FAILURES=$((CRITICAL_FAILURES + 1))
          fi
          
          # Non-critical checks
          if [[ "${{ needs.type-check.result }}" == "failure" ]]; then
            echo "⚠️  WARNING: Type checking failed (non-blocking)"
          fi
          
          if [[ "${{ needs.dependency-check.result }}" == "failure" ]]; then
            echo "⚠️  WARNING: Dependency security check failed (non-blocking)"
          fi
          
          if [[ "${{ needs.docker-test.result }}" == "failure" ]]; then
            echo "⚠️  WARNING: Docker test failed (non-blocking)"
          fi
          
          # Final result
          if [[ $CRITICAL_FAILURES -gt 0 ]]; then
            echo "❌ CI pipeline failed - $CRITICAL_FAILURES critical checks failed"
            echo "💡 Core application functionality may be broken"
            exit 1
          else
            echo "✅ CI pipeline passed - all critical checks successful"
            echo "🚀 Application is ready for deployment"
          fi

      - name: Post status comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const results = {
              'lint-and-format': '${{ needs.lint-and-format.result }}',
              'test': '${{ needs.test.result }}',
              'type-check': '${{ needs.type-check.result }}',
              'dependency-check': '${{ needs.dependency-check.result }}',
              'build-test': '${{ needs.build-test.result }}',
              'docker-test': '${{ needs.docker-test.result }}'
            };
            
            const critical = ['lint-and-format', 'test', 'build-test'];
            const warnings = ['type-check', 'dependency-check', 'docker-test'];
            
            let message = '## 🚀 CI Pipeline Results\n\n';
            
            message += '### Critical Checks\n';
            for (const job of critical) {
              const result = results[job];
              const emoji = result === 'success' ? '✅' : result === 'failure' ? '❌' : result === 'skipped' ? '⏭️' : '🔄';
              message += `${emoji} **${job}**: ${result}\n`;
            }
            
            message += '\n### Additional Checks\n';
            for (const job of warnings) {
              const result = results[job];
              const emoji = result === 'success' ? '✅' : result === 'failure' ? '⚠️' : result === 'skipped' ? '⏭️' : '🔄';
              message += `${emoji} **${job}**: ${result}\n`;
            }
            
            const criticalPassed = critical.every(job => results[job] === 'success' || results[job] === 'skipped');
            message += criticalPassed ? '\n🎉 **Status**: Ready to merge!' : '\n🚫 **Status**: Critical issues found';
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: message
            });