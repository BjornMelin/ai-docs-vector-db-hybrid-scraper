name: CI Performance Monitor

on:
  workflow_run:
    workflows: ["Continuous Integration", "Continuous Integration (Optimized)", "Fast Check (Pre-commit Style)"]
    types:
      - completed
  schedule:
    # Run weekly to track CI performance trends
    - cron: '0 8 * * 1'  # Every Monday at 8 AM UTC
  workflow_dispatch:

jobs:
  analyze-ci-performance:
    name: Analyze CI Performance
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion != 'cancelled'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Install dependencies
        run: |
          pip install requests python-dateutil matplotlib pandas

      - name: Analyze workflow performance
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python - << 'EOF'
          import requests
          import json
          import datetime
          from dateutil import parser
          import statistics
          
          # GitHub API setup
          headers = {
              'Authorization': f'token ${{ secrets.GITHUB_TOKEN }}',
              'Accept': 'application/vnd.github.v3+json'
          }
          
          repo = '${{ github.repository }}'
          
          # Get recent workflow runs
          workflows = ['ci.yml', 'ci-optimized.yml', 'fast-check.yml']
          
          performance_data = {}
          
          for workflow in workflows:
              url = f'https://api.github.com/repos/{repo}/actions/workflows/{workflow}/runs'
              params = {'per_page': 50, 'status': 'completed'}
              
              try:
                  response = requests.get(url, headers=headers, params=params)
                  response.raise_for_status()
                  
                  runs = response.json()['workflow_runs']
                  
                  durations = []
                  success_rate = 0
                  total_runs = len(runs)
                  
                  for run in runs:
                      if run['conclusion'] in ['success', 'failure']:
                          # Calculate duration
                          start = parser.parse(run['created_at'])
                          end = parser.parse(run['updated_at'])
                          duration = (end - start).total_seconds() / 60  # minutes
                          durations.append(duration)
                          
                          if run['conclusion'] == 'success':
                              success_rate += 1
                  
                  if durations:
                      performance_data[workflow] = {
                          'avg_duration': statistics.mean(durations),
                          'median_duration': statistics.median(durations),
                          'max_duration': max(durations),
                          'min_duration': min(durations),
                          'success_rate': (success_rate / total_runs * 100) if total_runs > 0 else 0,
                          'total_runs': total_runs
                      }
                      
              except Exception as e:
                  print(f"Error analyzing {workflow}: {e}")
                  continue
          
          # Generate performance report
          report = "# CI Pipeline Performance Report\n\n"
          report += f"Generated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n"
          
          if performance_data:
              report += "## Performance Summary\n\n"
              report += "| Workflow | Avg Duration (min) | Median (min) | Success Rate | Total Runs |\n"
              report += "|----------|-------------------|--------------|--------------|------------|\n"
              
              for workflow, data in performance_data.items():
                  report += f"| {workflow} | {data['avg_duration']:.1f} | {data['median_duration']:.1f} | {data['success_rate']:.1f}% | {data['total_runs']} |\n"
              
              report += "\n## Detailed Analysis\n\n"
              
              for workflow, data in performance_data.items():
                  report += f"### {workflow}\n\n"
                  report += f"- **Average Duration**: {data['avg_duration']:.1f} minutes\n"
                  report += f"- **Median Duration**: {data['median_duration']:.1f} minutes\n"
                  report += f"- **Range**: {data['min_duration']:.1f} - {data['max_duration']:.1f} minutes\n"
                  report += f"- **Success Rate**: {data['success_rate']:.1f}%\n"
                  report += f"- **Total Runs Analyzed**: {data['total_runs']}\n\n"
              
              # Performance recommendations
              report += "## Performance Recommendations\n\n"
              
              # Find slowest workflow
              slowest = max(performance_data.items(), key=lambda x: x[1]['avg_duration'])
              fastest = min(performance_data.items(), key=lambda x: x[1]['avg_duration'])
              
              report += f"### üêå Slowest Pipeline: {slowest[0]}\n"
              report += f"Average duration: {slowest[1]['avg_duration']:.1f} minutes\n\n"
              
              report += f"### ‚ö° Fastest Pipeline: {fastest[0]}\n"
              report += f"Average duration: {fastest[1]['avg_duration']:.1f} minutes\n\n"
              
              # Success rate analysis
              lowest_success = min(performance_data.items(), key=lambda x: x[1]['success_rate'])
              if lowest_success[1]['success_rate'] < 90:
                  report += f"### ‚ö†Ô∏è Reliability Issue: {lowest_success[0]}\n"
                  report += f"Success rate: {lowest_success[1]['success_rate']:.1f}%\n"
                  report += "Consider investigating common failure patterns.\n\n"
              
              # General recommendations
              report += "### üí° Optimization Suggestions\n\n"
              
              if slowest[1]['avg_duration'] > 15:
                  report += "- Consider reducing test matrix size for PR builds\n"
                  report += "- Implement more aggressive caching strategies\n"
                  report += "- Split long-running jobs into parallel stages\n"
              
              if any(data['avg_duration'] > 10 for data in performance_data.values()):
                  report += "- Review dependency installation time\n"
                  report += "- Consider using pre-built Docker images\n"
                  report += "- Optimize test execution with pytest-xdist\n"
              
              report += "- Monitor cache hit rates and optimize cache keys\n"
              report += "- Use fail-fast strategies for immediate feedback\n"
              
          else:
              report += "No performance data available for analysis.\n"
          
          # Write report
          with open('ci-performance-report.md', 'w') as f:
              f.write(report)
          
          print("Performance analysis complete!")
          print(report)
          EOF

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        with:
          name: ci-performance-report
          path: ci-performance-report.md
          retention-days: 30

      - name: Create performance issue (if needed)
        if: github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const report = fs.readFileSync('ci-performance-report.md', 'utf8');
              
              // Check if we should create a performance issue
              const avgDurationMatch = report.match(/Average Duration.*?(\d+\.\d+) minutes/);
              const successRateMatch = report.match(/Success Rate.*?(\d+\.\d+)%/);
              
              let shouldCreateIssue = false;
              let issueTitle = 'üîß CI Performance Review';
              let issueBody = report + '\n\n---\n\n';
              
              if (avgDurationMatch && parseFloat(avgDurationMatch[1]) > 20) {
                shouldCreateIssue = true;
                issueTitle = 'üêå CI Pipeline Performance Issue - High Duration';
                issueBody += '**Action Required**: CI duration exceeds 20 minutes average.\n';
              }
              
              if (successRateMatch && parseFloat(successRateMatch[1]) < 85) {
                shouldCreateIssue = true;
                issueTitle = '‚ùå CI Pipeline Reliability Issue - Low Success Rate';
                issueBody += '**Action Required**: CI success rate below 85%.\n';
              }
              
              if (shouldCreateIssue) {
                issueBody += '\nPlease review the performance report and implement optimization strategies.\n';
                issueBody += '\n### Quick Wins\n';
                issueBody += '- Review cache hit rates\n';
                issueBody += '- Optimize test matrix\n';
                issueBody += '- Check for flaky tests\n';
                issueBody += '- Consider dependency optimization\n';
                
                // Check if similar issue already exists
                const { data: issues } = await github.rest.issues.listForRepo({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  labels: 'ci-performance',
                  state: 'open'
                });
                
                if (issues.length === 0) {
                  await github.rest.issues.create({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    title: issueTitle,
                    body: issueBody,
                    labels: ['ci-performance', 'automation', 'enhancement']
                  });
                  
                  console.log('Created CI performance issue');
                } else {
                  console.log('CI performance issue already exists');
                }
              } else {
                console.log('CI performance is within acceptable limits');
              }
              
            } catch (error) {
              console.log('Error creating performance issue:', error);
            }

  cache-analysis:
    name: Analyze Cache Performance
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Analyze cache efficiency
        run: |
          echo "# Cache Performance Analysis" > cache-analysis.md
          echo "" >> cache-analysis.md
          echo "## Current Cache Strategy" >> cache-analysis.md
          echo "" >> cache-analysis.md
          
          # Analyze cache configuration
          if [ -f ".github/workflows/shared/cache-config.yml" ]; then
            echo "### Configured Caches" >> cache-analysis.md
            echo "" >> cache-analysis.md
            echo "\`\`\`yaml" >> cache-analysis.md
            cat .github/workflows/shared/cache-config.yml >> cache-analysis.md
            echo "\`\`\`" >> cache-analysis.md
            echo "" >> cache-analysis.md
          fi
          
          echo "### Cache Optimization Recommendations" >> cache-analysis.md
          echo "" >> cache-analysis.md
          echo "1. **Python Dependencies**: Use uv cache with lock file hashing" >> cache-analysis.md
          echo "2. **Browser Dependencies**: Cache Playwright browsers across jobs" >> cache-analysis.md
          echo "3. **Build Artifacts**: Cache Docker layers and Sphinx builds" >> cache-analysis.md
          echo "4. **Tool Caches**: Cache ruff, pytest, and other development tools" >> cache-analysis.md
          echo "" >> cache-analysis.md
          
          echo "### Current Issues" >> cache-analysis.md
          echo "" >> cache-analysis.md
          
          # Check for potential cache issues
          if grep -r "cache.*miss" .github/workflows/ || true; then
            echo "- Cache misses detected in workflow logs" >> cache-analysis.md
          fi
          
          if find .github/workflows/ -name "*.yml" -exec grep -l "cache" {} \; | wc -l | grep -q "^[0-9]$"; then
            echo "- Multiple workflows using different cache keys" >> cache-analysis.md
          fi
          
          echo "- Consider implementing cache warming strategies" >> cache-analysis.md
          echo "- Monitor cache hit rates in workflow logs" >> cache-analysis.md

      - name: Upload cache analysis
        uses: actions/upload-artifact@v4
        with:
          name: cache-analysis
          path: cache-analysis.md
          retention-days: 30