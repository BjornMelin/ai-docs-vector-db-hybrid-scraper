name: Optimized CI with pytest-xdist

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Test type to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - fast
          - full

permissions:
  contents: read
  pull-requests: write
  checks: write

env:
  PYTHONUNBUFFERED: 1
  PYTHONDONTWRITEBYTECODE: 1
  UV_SYSTEM_PYTHON: 1
  UV_CACHE_COMPRESSION: 1
  # pytest-xdist optimization
  PYTEST_XDIST_AUTO_NUM_WORKERS: 4
  PYTEST_TIMEOUT: 300

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  test-matrix:
    name: Test Suite (${{ matrix.os }} / Python ${{ matrix.python-version }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 20
    
    strategy:
      fail-fast: false
      matrix:
        include:
          # PR testing - fast feedback
          - os: ubuntu-latest
            python-version: '3.12'
            test-type: 'fast'
            coverage: true
            if: github.event_name == 'pull_request'
          
          # Main branch - comprehensive
          - os: ubuntu-latest
            python-version: '3.12'
            test-type: 'all'
            coverage: true
            if: github.ref == 'refs/heads/main'
          
          - os: ubuntu-latest
            python-version: '3.11'
            test-type: 'unit'
            coverage: false
            if: github.ref == 'refs/heads/main'
          
          - os: windows-latest
            python-version: '3.12'
            test-type: 'unit'
            coverage: false
            if: github.ref == 'refs/heads/main'
          
          - os: macos-latest
            python-version: '3.12'
            test-type: 'unit'
            coverage: false
            if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python with uv
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true
          version: "latest"

      - name: Set up Python ${{ matrix.python-version }}
        run: uv python install ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          uv sync --frozen
          uv pip install pytest-xdist pytest-timeout pytest-cov

      - name: Create test directories
        shell: bash
        run: |
          mkdir -p tests/fixtures/{cache,data,logs,vectors,embeddings}
          mkdir -p logs cache data htmlcov test-results
          mkdir -p tests/fixtures/cache/worker_{gw0,gw1,gw2,gw3}
          mkdir -p tests/fixtures/data/worker_{gw0,gw1,gw2,gw3}
          mkdir -p tests/fixtures/logs/worker_{gw0,gw1,gw2,gw3}

      - name: Configure pytest-xdist for OS
        id: xdist-config
        shell: bash
        run: |
          if [ "${{ runner.os }}" == "Windows" ]; then
            echo "workers=2" >> $GITHUB_OUTPUT
            echo "dist=loadfile" >> $GITHUB_OUTPUT
            echo "timeout=600" >> $GITHUB_OUTPUT
          elif [ "${{ runner.os }}" == "macOS" ]; then
            echo "workers=3" >> $GITHUB_OUTPUT
            echo "dist=loadscope" >> $GITHUB_OUTPUT
            echo "timeout=300" >> $GITHUB_OUTPUT
          else
            echo "workers=4" >> $GITHUB_OUTPUT
            echo "dist=loadscope" >> $GITHUB_OUTPUT
            echo "timeout=300" >> $GITHUB_OUTPUT
          fi

      - name: Run optimized test suite
        env:
          COVERAGE_PARALLEL: true
          PYTEST_XDIST_WORKER_COUNT: ${{ steps.xdist-config.outputs.workers }}
        shell: bash
        run: |
          # Use our optimized test runner
          python scripts/run_ci_tests.py \
            --test-type "${{ matrix.test-type || 'all' }}" \
            ${{ matrix.coverage && '--coverage' || '' }} \
            --performance-report \
            --workers ${{ steps.xdist-config.outputs.workers }} \
            --timeout ${{ steps.xdist-config.outputs.timeout }} \
            -v

      - name: Combine coverage data
        if: matrix.coverage
        shell: bash
        run: |
          uv run coverage combine
          uv run coverage report
          uv run coverage xml

      - name: Upload coverage to Codecov
        if: matrix.coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: ${{ matrix.os }}-py${{ matrix.python-version }}
          fail_ci_if_error: false
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.os }}-py${{ matrix.python-version }}
          path: |
            test-results/
            test-performance*.json
            .coverage*
          retention-days: 7

      - name: Parse performance report
        if: always()
        shell: bash
        run: |
          if [ -f "test-performance*.json" ]; then
            echo "## Performance Report" >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
            cat test-performance*.json | jq '.summary' >> $GITHUB_STEP_SUMMARY
            echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
            
            # Extract recommendations
            echo "### Recommendations" >> $GITHUB_STEP_SUMMARY
            cat test-performance*.json | jq -r '.recommendations[]' | while read -r rec; do
              echo "- $rec" >> $GITHUB_STEP_SUMMARY
            done
          fi

  performance-analysis:
    name: Performance Analysis
    runs-on: ubuntu-latest
    needs: test-matrix
    if: always()
    
    steps:
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          pattern: test-results-*
          merge-multiple: true

      - name: Analyze test performance
        run: |
          echo "## Test Performance Summary" >> $GITHUB_STEP_SUMMARY
          
          # Combine all performance reports
          if ls test-performance*.json 1> /dev/null 2>&1; then
            python -c "
          import json
          import glob
          
          reports = []
          for file in glob.glob('test-performance*.json'):
              with open(file) as f:
                  reports.append(json.load(f))
          
          # Aggregate metrics
          total_tests = sum(r['metadata']['total_tests'] for r in reports)
          total_duration = sum(r['metadata']['total_duration'] for r in reports)
          total_workers = sum(r['metadata']['workers_used'] for r in reports)
          
          print(f'Total tests across all jobs: {total_tests}')
          print(f'Total duration: {total_duration:.2f}s')
          print(f'Total workers used: {total_workers}')
          print(f'Average time per test: {total_duration/total_tests:.3f}s')
          
          # Find slowest tests across all runs
          all_slow_tests = []
          for r in reports:
              all_slow_tests.extend(r.get('slow_tests', []))
          
          if all_slow_tests:
              print('\\nTop 5 slowest tests:')
              for test in sorted(all_slow_tests, key=lambda x: x['duration'], reverse=True)[:5]:
                  print(f\"  - {test['nodeid']}: {test['duration']:.2f}s\")
          " >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment on PR with performance insights
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const glob = require('glob');
            
            // Read performance reports
            const reports = glob.sync('test-performance*.json').map(file => 
              JSON.parse(fs.readFileSync(file, 'utf8'))
            );
            
            if (reports.length === 0) return;
            
            // Build comment
            let comment = '## ðŸš€ Test Performance Report\n\n';
            
            reports.forEach(report => {
              const meta = report.metadata;
              comment += `### ${meta.generated_at}\n`;
              comment += `- **Total Tests**: ${meta.total_tests}\n`;
              comment += `- **Duration**: ${meta.total_duration.toFixed(2)}s\n`;
              comment += `- **Workers**: ${meta.workers_used}\n`;
              comment += `- **Load Balance Score**: ${report.test_distribution.load_balance_score.toFixed(1)}%\n\n`;
              
              if (report.recommendations.length > 0) {
                comment += '**Recommendations:**\n';
                report.recommendations.forEach(rec => {
                  comment += `- ${rec}\n`;
                });
              }
            });
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });