{"query": "How should operators rotate API keys?", "expected_answer": "Generate a replacement key in the security portal, roll it out to dependent services, validate traffic, and immediately revoke the old key.", "expected_contexts": ["To rotate API keys, issue a new credential in the security portal, update all services to use the replacement, verify connectivity, and revoke the retired key as soon as validation passes."], "references": ["docs/security/security-essentials.md#api-key-rotation"], "metadata": {"collection": "golden_eval", "category": "security", "priority": "p0"}}
{"query": "What payload field is used to group search results by default?", "expected_answer": "Results are grouped by the doc_id payload unless the request overrides group_by.", "expected_contexts": ["Server-side grouping accounts collapse results that share the same doc_id payload value unless the caller supplies a custom group_by key."], "references": ["docs/query/evaluation-reference.md#grouping-defaults"], "metadata": {"collection": "golden_eval", "category": "retrieval", "priority": "p1"}}
{"query": "How does synonym expansion treat the term 'install'?", "expected_answer": "The synonym dictionary expands 'install' to include 'setup' and 'configure'.", "expected_contexts": ["Synonym expansion maps install to setup and configure so the retriever can match documents that describe the installation process with alternate verbs."], "references": ["docs/query/evaluation-reference.md#synonym-expansion"], "metadata": {"collection": "golden_eval", "category": "query", "priority": "p2"}}
{"query": "Which histogram tracks LangGraph stage latency?", "expected_answer": "The ml_app_rag_stage_latency_seconds histogram records retrieve, grade, and generate durations.", "expected_contexts": ["The rag stage latency histogram ml_app_rag_stage_latency_seconds captures retrieve, grade, and generate timings per collection so regressions are visible before they affect end users."], "references": ["docs/observability/rag-evaluation-metrics.md#stage-latency-histogram"], "metadata": {"collection": "golden_eval", "category": "observability", "priority": "p1"}}
{"query": "What does the compression ratio metric represent?", "expected_answer": "It records the fraction of tokens retained after contextual compression.", "expected_contexts": ["The compression ratio metric reports the proportion of tokens kept after contextual compression so we can confirm summarisation actually reduces prompt size."], "references": ["docs/observability/rag-evaluation-metrics.md#compression-metrics"], "metadata": {"collection": "golden_eval", "category": "observability", "priority": "p2"}}
{"query": "Where are evaluation gating thresholds documented?", "expected_answer": "Thresholds live in config/eval_budgets.yml and enforce similarity and retrieval minimums.", "expected_contexts": ["Evaluation gating thresholds reside in config/eval_budgets.yml and currently enforce minimum similarity, precision@k, and latency budgets for the budget CI lane."], "references": ["docs/testing/evaluation-harness.md#evaluation-budgets"], "metadata": {"collection": "golden_eval", "category": "evaluation", "priority": "p1"}}
{"query": "How many samples should semantic evaluation process by default?", "expected_answer": "Semantic evaluation caps RAGAS scoring at 25 samples unless the caller overrides it.", "expected_contexts": ["Semantic evaluation defaults to processing at most twenty five samples with RAGAS to keep API spend predictable unless the caller specifies a different cap."], "references": ["docs/testing/evaluation-harness.md#semantic-evaluation-limits-and-cost-controls"], "metadata": {"collection": "golden_eval", "category": "evaluation", "priority": "p2"}}
{"query": "Which metric names are included in the telemetry snapshot?", "expected_answer": "Only metrics declared in metrics_allowlist.json are exported in the snapshot.", "expected_contexts": ["The telemetry snapshot serialises only metrics listed in metrics_allowlist.json to keep artefacts small and stable between runs."], "references": ["docs/observability/rag-evaluation-metrics.md#metrics-allowlist-enforcement"], "metadata": {"collection": "golden_eval", "category": "observability", "priority": "p2"}}
{"query": "What collection name is used for regression evaluation?", "expected_answer": "Regression tests store documents in the golden_eval Qdrant collection.", "expected_contexts": ["The regression harness seeds the golden_eval Qdrant collection with curated documents that mirror the golden dataset cases."], "references": ["docs/testing/evaluation-harness.md#golden-dataset-and-collection"], "metadata": {"collection": "golden_eval", "category": "infrastructure", "priority": "p1"}}
{"query": "How do we seed the golden Qdrant collection locally?", "expected_answer": "Run scripts/eval/seed_qdrant.py to recreate the golden_eval collection with deterministic points.", "expected_contexts": ["Use scripts/eval/seed_qdrant.py to recreate the golden_eval collection with deterministic points so evaluation runs are reproducible."], "references": ["docs/testing/evaluation-harness.md#seeding-qdrant"], "metadata": {"collection": "golden_eval", "category": "infrastructure", "priority": "p2"}}
{"query": "Which file records evaluation cost guardrails?", "expected_answer": "Cost guardrails are captured in config/eval_costs.yml alongside provider rate limits.", "expected_contexts": ["Config/eval_costs.yml documents the semantic evaluation cost guardrails and provider rate limits referenced by CI."], "references": ["docs/testing/evaluation-harness.md#semantic-evaluation-limits-and-cost-controls"], "metadata": {"collection": "golden_eval", "category": "evaluation", "priority": "p2"}}
{"query": "What log message confirms semantic evaluation is enabled?", "expected_answer": "The harness logs that semantic evaluation is enabled and API usage limits should be configured.", "expected_contexts": ["When semantic evaluation is active the harness logs that API usage limits should be configured to avoid runaway spend."], "references": ["docs/observability/rag-evaluation-metrics.md#evaluation-logging"], "metadata": {"collection": "golden_eval", "category": "observability", "priority": "p3"}}
