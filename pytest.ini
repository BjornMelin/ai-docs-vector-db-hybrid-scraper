[pytest]
# Test Infrastructure Modernization - Optimized Configuration for 2025

# Core collection settings with performance optimizations
testpaths = tests
python_files = test_*.py *_test.py
python_functions = test_*
python_classes = Test* *Test *Tests

# Collection optimization
norecursedirs = .git .tox dist build *.egg .venv __pycache__ .mypy_cache .pytest_cache .ruff_cache

# Output formatting for fast feedback with modern optimizations
addopts = 
    --tb=short
    --strict-markers
    --disable-warnings
    --no-header
    -ra
    --maxfail=10
    --durations=10
    --strict-config
    --showlocals
    -p no:hypothesispytest
    --import-mode=importlib

# Marker definitions for test categorization
markers =
    # Speed-based categories
    slow: marks tests as slow running (> 5 seconds)
    fast: marks tests as fast running (< 1 second)
    smoke: marks tests as smoke tests
    
    # Functional categories
    unit: marks tests as unit tests
    integration: marks tests as integration tests
    performance: marks tests as performance/benchmark tests
    e2e: marks tests as end-to-end tests
    contract: marks tests as contract tests
    consumer_driven: marks tests as consumer-driven contract tests
    deployment: marks tests related to deployment
    pipeline: marks tests related to deployment pipeline
    infrastructure: marks tests related to infrastructure
    blue_green: marks tests related to blue-green deployment
    security: marks tests as security tests
    disaster_recovery: marks tests related to disaster recovery
    mcp: marks tests related to Model Context Protocol
    browser_monitoring: marks tests for browser automation monitoring
    database_pooling: marks tests for database connection pooling
    environment: marks tests related to environment configuration
    
    # AI/ML specific markers
    ai: marks tests as AI/ML specific
    ai_ml: marks tests as AI/ML specific
    embedding: marks tests as embedding-related
    vector_db: marks tests as vector database related
    rag: marks tests as RAG system related
    
    # Security testing markers
    vulnerability: marks tests as vulnerability tests
    penetration: marks tests as penetration tests
    authentication: marks tests as authentication tests
    authorization: marks tests as authorization tests
    compliance: marks tests as compliance tests
    data_protection: marks tests as data protection tests
    encryption: marks tests as encryption tests
    injection_prevention: marks tests as injection prevention tests
    pii_detection: marks tests as PII detection tests
    prompt_injection: marks tests as prompt injection tests
    rbac: marks tests as role-based access control tests
    zero_vulnerability: marks tests as zero vulnerability tests
    enterprise_grade: marks tests as enterprise grade tests
    input_validation: marks tests as input validation tests
    vulnerability_scan: marks tests as vulnerability scan tests
    penetration_test: marks tests as penetration tests
    owasp: marks tests as OWASP compliance tests
    injection: marks tests as injection tests
    auth: marks tests as authentication tests
    rate_limit: marks tests as rate limiting tests
    
    # Environment categories
    browser: marks tests requiring browser automation
    network: marks tests requiring network access
    database: marks tests requiring database connection
    
    # Platform categories
    windows: marks tests that should only run on Windows
    macos: marks tests that should only run on macOS
    linux: marks tests that should only run on Linux
    unix: marks tests that should run on Unix-like systems
    
    # Execution context
    ci_only: marks tests that should only run in CI
    local_only: marks tests that should only run locally
    
    # Test quality
    hypothesis: marks property-based tests using Hypothesis
    asyncio: marks async tests
    benchmark: marks benchmark tests
    
    # Deployment and reporting markers
    post_deployment: marks tests as post-deployment tests
    reporting: marks tests as reporting tests
    performance_critical: marks tests as performance critical tests
    smoke: marks tests as smoke tests
    
    # Additional markers
    ai: marks tests as AI/ML specific tests
    ai_ml: marks tests as AI/ML specific tests
    rag: marks tests as RAG system tests
    query_processing: marks tests as query processing tests
    observability: marks tests as observability tests
    monitoring: marks tests as monitoring tests
    service: marks tests as service tests
    config: marks tests as configuration tests
    models: marks tests as model tests
    modern: marks tests using modern patterns
    cli: marks tests as CLI tests
    component: marks tests as component tests
    transformation_validation: marks tests as transformation validation tests
    framework_resolution: marks tests as framework resolution tests
    compatibility: marks tests as compatibility tests
    respx_compatibility: marks tests as respx compatibility tests
    dependency_injection: marks tests as dependency injection tests
    async_framework: marks tests as async framework tests
    enterprise_grade: marks tests as enterprise grade tests
    
    # Performance markers
    throughput: marks tests as throughput tests
    throughput_validation: marks tests as throughput validation tests
    latency_validation: marks tests as latency validation tests
    cpu: marks tests as CPU tests
    memory: marks tests as memory tests
    scalability: marks tests as scalability tests
    baseline: marks tests as baseline tests
    comparison: marks tests as comparison tests
    
    # Load testing markers
    load: marks tests as load tests
    stress: marks tests as stress tests
    spike: marks tests as spike tests
    endurance: marks tests as endurance tests
    volume: marks tests as volume tests
    
    # Chaos testing markers
    chaos: marks tests as chaos engineering tests
    fault_injection: marks tests as fault injection tests
    resilience: marks tests as resilience tests
    failure_scenarios: marks tests as failure scenario tests
    network_chaos: marks tests as network chaos tests
    resource_exhaustion: marks tests as resource exhaustion tests
    
    # Security markers
    vulnerability: marks tests as vulnerability tests
    zero_vulnerability: marks tests as zero vulnerability tests
    authentication: marks tests as authentication tests
    authorization: marks tests as authorization tests
    rbac: marks tests as RBAC tests
    encryption: marks tests as encryption tests
    data_protection: marks tests as data protection tests
    data_sanitization: marks tests as data sanitization tests
    input_validation: marks tests as input validation tests
    injection_prevention: marks tests as injection prevention tests
    prompt_injection: marks tests as prompt injection tests
    pii_detection: marks tests as PII detection tests
    
    # Accessibility markers
    accessibility: marks tests as accessibility tests
    a11y: marks tests as general accessibility tests
    wcag: marks tests as WCAG compliance tests
    screen_reader: marks tests as screen reader tests
    keyboard_navigation: marks tests as keyboard navigation tests
    color_contrast: marks tests as color contrast tests
    aria: marks tests as ARIA attributes tests
    visual: marks tests as visual tests
    responsive: marks tests as responsive tests
    
    # Contract testing markers
    api_contract: marks tests as API contract tests
    schema_validation: marks tests as schema validation tests
    pact: marks tests as Pact contract tests
    openapi: marks tests as OpenAPI contract tests
    consumer_driven: marks tests as consumer-driven contract tests
    
    # Property-based testing markers
    property: marks tests as property-based tests
    property_based: marks tests as property-based tests

# Timeout settings with method-specific overrides
# Note: These require pytest-timeout plugin
# timeout = 120
# timeout_method = thread
# timeout_func_only = true

# Parallel execution
# Note: -n auto will be added by test runner scripts for parallel execution

# Async test configuration  
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Logging during tests
log_cli = false
log_cli_level = WARNING
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Warnings filter with performance considerations
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning:.*playwright.*
    ignore::UserWarning:.*fastembed.*
    ignore::ResourceWarning
    ignore::pytest.PytestUnraisableExceptionWarning
    error::pytest.PytestUnhandledThreadExceptionWarning
    
# Doctests configuration
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL ELLIPSIS

# Test discovery optimization
consider_namespace_packages = true

# Console output settings
console_output_style = progress