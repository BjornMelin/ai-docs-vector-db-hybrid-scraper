# Task ID: 30
# Title: FastEmbed GPU acceleration - Enable CUDA-based embeddings via fastembed-gpu for 5-10× vectorisation speed
# Status: pending
# Dependencies: 21, 22
# Priority: medium
# Description: Upgrade embedding generation to use fastembed-gpu with CUDA acceleration to achieve 5-10× speed improvements in document vectorization and semantic search operations.
# Details:
1. Install and configure fastembed-gpu dependency:
   - Add fastembed-gpu>=0.3.0 to pyproject.toml dependencies
   - Ensure CUDA runtime compatibility (CUDA 11.8+ or 12.x)
   - Configure GPU memory allocation and device selection
   - Add fallback to CPU-based fastembed if GPU unavailable

2. Update embedding service architecture:
   - Modify src/services/embeddings/manager.py to use fastembed-gpu providers
   - Implement GPU device detection and automatic fallback logic
   - Configure batch processing optimized for GPU memory constraints
   - Add GPU memory monitoring and cleanup mechanisms

3. Optimize embedding generation pipeline:
   - Implement dynamic batch sizing based on available GPU memory
   - Add GPU utilization monitoring and performance metrics
   - Configure mixed-precision inference for additional speed gains
   - Implement embedding caching to avoid redundant GPU computations

4. Update configuration and environment setup:
   - Add GPU-specific configuration options to settings
   - Update Docker configuration for CUDA runtime support
   - Add GPU requirements detection and validation
   - Configure environment variables for GPU memory limits

# Test Strategy:
1. Performance benchmarking validation:
   - Create comprehensive embedding generation benchmark comparing CPU vs GPU performance
   - Measure vectorization speed with datasets of 1K, 10K, and 100K documents
   - Verify 5-10× speed improvement target across different document sizes
   - Test memory usage patterns and GPU utilization efficiency

2. Functionality and fallback testing:
   - Test GPU detection and automatic fallback to CPU when GPU unavailable
   - Validate embedding quality and consistency between CPU and GPU implementations
   - Test batch processing with various document sizes and types
   - Verify memory cleanup and GPU resource management

3. Integration and compatibility testing:
   - Test integration with existing vector database operations
   - Validate semantic search accuracy with GPU-generated embeddings
   - Test concurrent embedding generation and resource sharing
   - Verify compatibility across different CUDA versions and GPU architectures

# Subtasks:
## 1. Install and Configure fastembed-gpu with CUDA Support [pending]
### Dependencies: None
### Description: Install the fastembed-gpu package, ensure CUDA runtime compatibility (CUDA 11.8+ or 12.x), and configure the environment for GPU acceleration.
### Details:
Uninstall any existing fastembed or onnxruntime packages to avoid conflicts, then install fastembed-gpu. Verify CUDA and cuDNN versions using nvidia-smi and nvcc --version. Ensure the correct CUDA toolkit and cuDNN libraries are present and accessible in the environment.

## 2. Update Embedding Service Architecture for GPU Execution [pending]
### Dependencies: 30.1
### Description: Modify the embedding service codebase to utilize fastembed-gpu providers, implement GPU device detection, and add fallback logic to CPU-based embedding if GPU is unavailable.
### Details:
Refactor src/services/embeddings/manager.py to select the CUDAExecutionProvider when a compatible GPU is detected. Implement logic to automatically revert to CPU-based fastembed if no GPU is available or if initialization fails.

## 3. Optimize Embedding Generation Pipeline for GPU [pending]
### Dependencies: 30.2
### Description: Enhance the embedding pipeline to leverage GPU capabilities, including dynamic batch sizing, mixed-precision inference, and embedding caching.
### Details:
Implement logic to adjust batch sizes based on available GPU memory, enable mixed-precision inference for speed improvements, and add caching to avoid redundant computations. Integrate GPU utilization and performance metrics collection.

## 4. Update Configuration and Environment Setup for GPU [pending]
### Dependencies: 30.3
### Description: Add GPU-specific configuration options, update Docker and environment setup for CUDA support, and validate GPU requirements at startup.
### Details:
Extend configuration files to include GPU memory limits and device selection. Update Dockerfiles to install CUDA runtime and cuDNN libraries. Implement startup checks to validate GPU presence and compatibility.

## 5. Implement Monitoring, Logging, and Fallback Mechanisms [pending]
### Dependencies: 30.4
### Description: Integrate GPU memory monitoring, error logging, and automatic cleanup, ensuring robust fallback to CPU in case of GPU errors or resource exhaustion.
### Details:
Add real-time monitoring of GPU memory usage, log any errors or warnings related to GPU execution, and implement cleanup routines to release GPU resources. Ensure that the system gracefully falls back to CPU-based embedding if GPU execution fails at runtime.

