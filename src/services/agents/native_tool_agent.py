"""Native Pydantic-AI Tool Agent for autonomous tool composition.

This module replaces the legacy ToolCompositionEngine (950 lines) with
intelligent, autonomous tool orchestration using native Pydantic-AI patterns.
Reduces complexity while adding true agentic capabilities.
"""

import asyncio
import logging
import time
from dataclasses import dataclass
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple
from uuid import uuid4

from pydantic import BaseModel, Field


try:
    from pydantic_ai import Agent, RunContext
    from pydantic_ai.tools import Tool

    PYDANTIC_AI_AVAILABLE = True
except ImportError:
    PYDANTIC_AI_AVAILABLE = False
    Agent = None
    RunContext = None
    Tool = None

from .core import BaseAgent, BaseAgentDependencies


logger = logging.getLogger(__name__)


class ToolCapability(Enum):
    """Tool capability categories for intelligent matching."""

    SEARCH = "search"
    RETRIEVAL = "retrieval"
    GENERATION = "generation"
    ANALYSIS = "analysis"
    FILTERING = "filtering"
    RANKING = "ranking"
    EMBEDDING = "embedding"
    CLASSIFICATION = "classification"


class PerformanceProfile(BaseModel):
    """Performance characteristics for tool selection."""

    latency_ms: float = Field(..., description="Expected latency in milliseconds")
    accuracy_score: float = Field(..., description="Accuracy score (0-1)")
    cost_factor: float = Field(..., description="Relative cost factor")
    reliability: float = Field(..., description="Reliability score (0-1)")
    scalability: str = Field(..., description="Scalability characteristics")


class ToolMetadata(BaseModel):
    """Enhanced tool metadata for autonomous selection."""

    name: str
    capabilities: list[ToolCapability]
    performance: PerformanceProfile
    dependencies: list[str] = Field(default_factory=list)
    input_schema: dict[str, Any] = Field(default_factory=dict)
    output_schema: dict[str, Any] = Field(default_factory=dict)
    usage_patterns: list[str] = Field(default_factory=list)
    integration_health: float = Field(
        default=1.0, description="Integration health score"
    )


class TaskRequirements(BaseModel):
    """Requirements for autonomous task analysis."""

    primary_capability: ToolCapability
    secondary_capabilities: list[ToolCapability] = Field(default_factory=list)
    performance_priority: str = Field(
        default="balanced"
    )  # speed, quality, cost, balanced
    max_latency_ms: float | None = None
    min_accuracy: float | None = None
    max_cost: float | None = None
    context: dict[str, Any] = Field(default_factory=dict)


class ExecutionPlan(BaseModel):
    """Autonomous execution plan generated by the agent."""

    plan_id: str = Field(default_factory=lambda: str(uuid4()))
    selected_tools: list[str]
    execution_strategy: str  # sequential, parallel, hybrid
    expected_latency_ms: float
    confidence_score: float
    reasoning: str
    fallback_tools: list[str] = Field(default_factory=list)


class ExecutionResult(BaseModel):
    """Result from autonomous tool execution."""

    plan_id: str
    success: bool
    results: dict[str, Any] = Field(default_factory=dict)
    actual_latency_ms: float
    tools_executed: list[str]
    performance_metrics: dict[str, float] = Field(default_factory=dict)
    error_details: str | None = None
    learned_insights: dict[str, Any] = Field(default_factory=dict)


class NativeToolAgent(BaseAgent):
    """Autonomous tool composition agent using native Pydantic-AI patterns.

    This agent replaces the 950-line ToolCompositionEngine with intelligent,
    autonomous tool orchestration that learns and adapts over time.
    """

    def __init__(self, model: str = "gpt-4", temperature: float = 0.1):
        """Initialize the native tool agent.

        Args:
            model: LLM model for decision making
            temperature: Generation temperature for tool selection
        """
        super().__init__(
            name="native_tool_agent",
            model=model,
            temperature=temperature,
            max_tokens=2000,
        )

        self.tool_registry: dict[str, ToolMetadata] = {}
        self.execution_history: list[ExecutionResult] = []
        self.learned_patterns: dict[str, Any] = {}

        if PYDANTIC_AI_AVAILABLE:
            self._setup_agent()

    def get_system_prompt(self) -> str:
        """Define the agent's autonomous tool orchestration behavior."""
        return """You are an autonomous tool orchestration agent with the following capabilities:

1. INTELLIGENT TOOL ANALYSIS
   - Analyze task requirements and user intent
   - Assess tool capabilities and performance profiles
   - Select optimal tool combinations based on context

2. AUTONOMOUS DECISION MAKING  
   - Choose execution strategies (sequential, parallel, hybrid)
   - Balance performance, quality, and cost constraints
   - Make real-time adaptations based on results

3. SELF-HEALING OPERATIONS
   - Detect tool failures and performance degradation
   - Automatically switch to fallback tools
   - Learn from failures to improve future decisions

4. PERFORMANCE OPTIMIZATION
   - Track tool performance across executions
   - Identify usage patterns and optimization opportunities
   - Continuously improve selection algorithms

Your goal is to provide the most effective tool orchestration for each task while
learning and adapting to optimize future performance. Always explain your reasoning
and provide confidence scores for your decisions.

Available tools will be dynamically registered and you'll have access to their
real-time performance metrics and health status."""

    def _setup_agent(self) -> None:
        """Set up the Pydantic-AI agent with tool orchestration capabilities."""
        if not PYDANTIC_AI_AVAILABLE:
            return

        @self.agent.tool_plain
        async def analyze_task_requirements(
            task_description: str, constraints: dict[str, Any]
        ) -> TaskRequirements:
            """Analyze a task to determine required capabilities and constraints."""
            return await self._analyze_task_requirements(task_description, constraints)

        @self.agent.tool_plain
        async def select_optimal_tools(
            requirements: TaskRequirements, available_tools: list[str]
        ) -> ExecutionPlan:
            """Select the optimal tool combination for the given requirements."""
            return await self._select_optimal_tools(requirements, available_tools)

        @self.agent.tool_plain
        async def execute_tool_plan(
            plan: ExecutionPlan, input_data: dict[str, Any]
        ) -> ExecutionResult:
            """Execute the planned tool orchestration."""
            return await self._execute_tool_plan(plan, input_data)

        @self.agent.tool_plain
        async def learn_from_execution(
            result: ExecutionResult, original_requirements: TaskRequirements
        ) -> dict[str, Any]:
            """Learn from execution results to improve future decisions."""
            return await self._learn_from_execution(result, original_requirements)

    async def initialize_tools(self, deps: BaseAgentDependencies) -> None:
        """Initialize and discover available tools autonomously."""
        # Auto-discover MCP tools
        await self._discover_mcp_tools(deps)

        # Initialize tool health monitoring
        await self._setup_tool_monitoring(deps)

        # Load learned patterns from previous sessions
        await self._load_learned_patterns(deps)

        logger.info(f"NativeToolAgent initialized with {len(self.tool_registry)} tools")

    async def orchestrate_tools(
        self, task: str, constraints: dict[str, Any], deps: BaseAgentDependencies
    ) -> ExecutionResult:
        """Main entry point for autonomous tool orchestration.

        Args:
            task: Task description or query
            constraints: Performance and quality constraints
            deps: Agent dependencies

        Returns:
            Execution result with performance metrics and insights
        """
        if not self._initialized:
            await self.initialize(deps)

        start_time = time.time()

        try:
            # Step 1: Analyze task requirements
            requirements = await self._analyze_task_requirements(task, constraints)

            # Step 2: Select optimal tools
            available_tools = list(self.tool_registry.keys())
            execution_plan = await self._select_optimal_tools(
                requirements, available_tools
            )

            # Step 3: Execute the plan
            input_data = {"task": task, "constraints": constraints}
            result = await self._execute_tool_plan(execution_plan, input_data)

            # Step 4: Learn from the execution
            insights = await self._learn_from_execution(result, requirements)
            result.learned_insights = insights

            # Update session state
            deps.session_state.increment_tool_usage("native_tool_agent")
            deps.session_state.update_metrics(
                {
                    "last_orchestration_latency": result.actual_latency_ms,
                    "orchestration_success_rate": self._calculate_success_rate(),
                }
            )

            return result

        except Exception as e:
            logger.error(f"Tool orchestration failed: {e}", exc_info=True)

            return ExecutionResult(
                plan_id="failed",
                success=False,
                actual_latency_ms=(time.time() - start_time) * 1000,
                tools_executed=[],
                error_details=str(e),
            )

    async def _analyze_task_requirements(
        self, task: str, constraints: dict[str, Any]
    ) -> TaskRequirements:
        """Autonomously analyze task to determine requirements."""
        task_lower = task.lower()

        # Intelligent capability detection
        primary_capability = ToolCapability.SEARCH  # default
        secondary_capabilities = []

        if any(
            keyword in task_lower for keyword in ["search", "find", "locate", "lookup"]
        ):
            primary_capability = ToolCapability.SEARCH
            secondary_capabilities.append(ToolCapability.RANKING)

        elif any(
            keyword in task_lower
            for keyword in ["generate", "create", "write", "compose"]
        ):
            primary_capability = ToolCapability.GENERATION
            secondary_capabilities.extend(
                [ToolCapability.RETRIEVAL, ToolCapability.SEARCH]
            )

        elif any(
            keyword in task_lower
            for keyword in ["analyze", "evaluate", "assess", "examine"]
        ):
            primary_capability = ToolCapability.ANALYSIS
            secondary_capabilities.extend(
                [ToolCapability.RETRIEVAL, ToolCapability.CLASSIFICATION]
            )

        elif any(keyword in task_lower for keyword in ["filter", "refine", "narrow"]):
            primary_capability = ToolCapability.FILTERING
            secondary_capabilities.append(ToolCapability.RANKING)

        # Determine performance priority from constraints and task complexity
        performance_priority = "balanced"
        if constraints.get("max_latency_ms", 5000) < 1000:
            performance_priority = "speed"
        elif any(
            keyword in task_lower
            for keyword in ["comprehensive", "detailed", "thorough"]
        ):
            performance_priority = "quality"
        elif constraints.get("max_cost", 1.0) < 0.1:
            performance_priority = "cost"

        return TaskRequirements(
            primary_capability=primary_capability,
            secondary_capabilities=secondary_capabilities,
            performance_priority=performance_priority,
            max_latency_ms=constraints.get("max_latency_ms"),
            min_accuracy=constraints.get("min_accuracy"),
            max_cost=constraints.get("max_cost"),
            context={
                "original_task": task,
                "task_complexity": self._assess_complexity(task),
            },
        )

    async def _select_optimal_tools(
        self, requirements: TaskRequirements, available_tools: list[str]
    ) -> ExecutionPlan:
        """Intelligently select optimal tool combination."""

        # Score tools based on capability match and performance
        tool_scores = {}
        for tool_name in available_tools:
            if tool_name not in self.tool_registry:
                continue

            metadata = self.tool_registry[tool_name]
            score = self._calculate_tool_score(metadata, requirements)
            tool_scores[tool_name] = score

        # Select primary tool
        sorted_tools = sorted(tool_scores.items(), key=lambda x: x[1], reverse=True)
        selected_tools = []

        if sorted_tools:
            selected_tools.append(sorted_tools[0][0])

            # Add complementary tools for secondary capabilities
            for capability in requirements.secondary_capabilities:
                for tool_name, metadata in self.tool_registry.items():
                    if (
                        capability in metadata.capabilities
                        and tool_name not in selected_tools
                        and len(selected_tools) < 3
                    ):  # Limit tool chain length
                        selected_tools.append(tool_name)
                        break

        # Determine execution strategy
        strategy = "sequential"
        if len(selected_tools) > 1:
            if requirements.performance_priority == "speed":
                strategy = "parallel"
            elif any("analysis" in t or "classification" in t for t in selected_tools):
                strategy = "hybrid"

        # Calculate expected metrics
        expected_latency = sum(
            self.tool_registry[tool].performance.latency_ms
            for tool in selected_tools
            if tool in self.tool_registry
        )

        if strategy == "parallel":
            expected_latency = (
                max(
                    self.tool_registry[tool].performance.latency_ms
                    for tool in selected_tools
                    if tool in self.tool_registry
                )
                if selected_tools
                else 0
            )

        confidence = self._calculate_plan_confidence(selected_tools, requirements)

        # Generate reasoning
        reasoning = f"Selected {len(selected_tools)} tools for {requirements.primary_capability.value} task. "
        reasoning += f"Strategy: {strategy} based on {requirements.performance_priority} priority. "
        reasoning += (
            f"Expected latency: {expected_latency:.0f}ms, confidence: {confidence:.2f}"
        )

        return ExecutionPlan(
            selected_tools=selected_tools,
            execution_strategy=strategy,
            expected_latency_ms=expected_latency,
            confidence_score=confidence,
            reasoning=reasoning,
            fallback_tools=self._select_fallback_tools(selected_tools, requirements),
        )

    async def _execute_tool_plan(
        self, plan: ExecutionPlan, input_data: dict[str, Any]
    ) -> ExecutionResult:
        """Execute the tool orchestration plan."""
        start_time = time.time()
        results = {}
        executed_tools = []
        performance_metrics = {}

        try:
            if plan.execution_strategy == "sequential":
                results = await self._execute_sequential(
                    plan.selected_tools, input_data
                )
            elif plan.execution_strategy == "parallel":
                results = await self._execute_parallel(plan.selected_tools, input_data)
            else:  # hybrid
                results = await self._execute_hybrid(plan.selected_tools, input_data)

            executed_tools = plan.selected_tools
            success = True

        except Exception as e:
            logger.warning(f"Primary execution failed: {e}, trying fallbacks")

            # Try fallback tools
            try:
                results = await self._execute_sequential(
                    plan.fallback_tools, input_data
                )
                executed_tools = plan.fallback_tools
                success = True
            except Exception as fallback_error:
                logger.error(f"Fallback execution also failed: {fallback_error}")
                results = {"error": str(fallback_error)}
                success = False

        actual_latency = (time.time() - start_time) * 1000

        # Calculate performance metrics
        if success:
            performance_metrics = {
                "latency_vs_expected": actual_latency
                / max(plan.expected_latency_ms, 1),
                "tool_efficiency": len(executed_tools)
                / max(len(plan.selected_tools), 1),
                "plan_accuracy": 1.0 if executed_tools == plan.selected_tools else 0.5,
            }

        result = ExecutionResult(
            plan_id=plan.plan_id,
            success=success,
            results=results,
            actual_latency_ms=actual_latency,
            tools_executed=executed_tools,
            performance_metrics=performance_metrics,
        )

        # Store for learning
        self.execution_history.append(result)
        if len(self.execution_history) > 1000:  # Keep recent history
            self.execution_history = self.execution_history[-1000:]

        return result

    async def _execute_sequential(
        self, tools: list[str], input_data: dict[str, Any]
    ) -> dict[str, Any]:
        """Execute tools sequentially."""
        results = {}
        context = input_data.copy()

        for tool_name in tools:
            # Mock execution - would integrate with actual tools
            result = await self._mock_tool_execution(tool_name, context)
            results[f"{tool_name}_result"] = result
            context.update(result)

        return results

    async def _execute_parallel(
        self, tools: list[str], input_data: dict[str, Any]
    ) -> dict[str, Any]:
        """Execute tools in parallel."""
        tasks = [
            self._mock_tool_execution(tool_name, input_data) for tool_name in tools
        ]

        parallel_results = await asyncio.gather(*tasks, return_exceptions=True)

        results = {}
        for i, (tool_name, result) in enumerate(
            zip(tools, parallel_results, strict=False)
        ):
            if isinstance(result, Exception):
                results[f"{tool_name}_error"] = str(result)
            else:
                results[f"{tool_name}_result"] = result

        return results

    async def _execute_hybrid(
        self, tools: list[str], input_data: dict[str, Any]
    ) -> dict[str, Any]:
        """Execute with hybrid sequential/parallel strategy."""
        # Execute first tool sequentially to get context
        primary_result = await self._mock_tool_execution(tools[0], input_data)
        results = {f"{tools[0]}_result": primary_result}

        # Execute remaining tools in parallel with enhanced context
        if len(tools) > 1:
            enhanced_context = input_data.copy()
            enhanced_context.update(primary_result)

            parallel_tasks = [
                self._mock_tool_execution(tool_name, enhanced_context)
                for tool_name in tools[1:]
            ]

            parallel_results = await asyncio.gather(
                *parallel_tasks, return_exceptions=True
            )

            for i, (tool_name, result) in enumerate(
                zip(tools[1:], parallel_results, strict=False)
            ):
                if isinstance(result, Exception):
                    results[f"{tool_name}_error"] = str(result)
                else:
                    results[f"{tool_name}_result"] = result

        return results

    async def _mock_tool_execution(
        self, tool_name: str, context: dict[str, Any]
    ) -> dict[str, Any]:
        """Mock tool execution - would be replaced with actual tool integration."""
        await asyncio.sleep(0.1)  # Simulate processing time

        return {
            "tool": tool_name,
            "result": f"Mock result from {tool_name}",
            "context_keys": list(context.keys()),
            "timestamp": time.time(),
        }

    # Implementation continues with discovery, monitoring, and learning methods...

    async def _discover_mcp_tools(self, deps: BaseAgentDependencies) -> None:
        """Auto-discover available MCP tools."""
        # Mock discovery - would integrate with actual MCP tool registry
        mock_tools = {
            "hybrid_search": ToolMetadata(
                name="hybrid_search",
                capabilities=[ToolCapability.SEARCH, ToolCapability.RETRIEVAL],
                performance=PerformanceProfile(
                    latency_ms=150.0,
                    accuracy_score=0.85,
                    cost_factor=0.02,
                    reliability=0.95,
                    scalability="high",
                ),
            ),
            "rag_generator": ToolMetadata(
                name="rag_generator",
                capabilities=[ToolCapability.GENERATION],
                performance=PerformanceProfile(
                    latency_ms=800.0,
                    accuracy_score=0.90,
                    cost_factor=0.10,
                    reliability=0.92,
                    scalability="medium",
                ),
            ),
            "content_analyzer": ToolMetadata(
                name="content_analyzer",
                capabilities=[ToolCapability.ANALYSIS, ToolCapability.CLASSIFICATION],
                performance=PerformanceProfile(
                    latency_ms=200.0,
                    accuracy_score=0.88,
                    cost_factor=0.03,
                    reliability=0.94,
                    scalability="high",
                ),
            ),
        }

        self.tool_registry.update(mock_tools)

    async def _setup_tool_monitoring(self, deps: BaseAgentDependencies) -> None:
        """Set up tool health monitoring."""
        # Would implement real-time tool health checking

    async def _load_learned_patterns(self, deps: BaseAgentDependencies) -> None:
        """Load previously learned patterns."""
        # Would load from persistent storage

    def _calculate_tool_score(
        self, metadata: ToolMetadata, requirements: TaskRequirements
    ) -> float:
        """Calculate tool fitness score for requirements."""
        score = 0.0

        # Capability match
        if requirements.primary_capability in metadata.capabilities:
            score += 10.0

        for secondary in requirements.secondary_capabilities:
            if secondary in metadata.capabilities:
                score += 3.0

        # Performance alignment
        if requirements.performance_priority == "speed":
            score += 5.0 / max(metadata.performance.latency_ms / 100.0, 1.0)
        elif requirements.performance_priority == "quality":
            score += metadata.performance.accuracy_score * 5.0
        elif requirements.performance_priority == "cost":
            score += 5.0 / max(metadata.performance.cost_factor * 100, 1.0)
        else:  # balanced
            score += metadata.performance.accuracy_score * 2.5 + 2.5 / max(
                metadata.performance.latency_ms / 100.0, 1.0
            )

        # Reliability factor
        score *= metadata.performance.reliability

        return score

    def _calculate_plan_confidence(
        self, selected_tools: list[str], requirements: TaskRequirements
    ) -> float:
        """Calculate confidence in the execution plan."""
        if not selected_tools:
            return 0.0

        # Base confidence from tool reliability
        reliabilities = [
            self.tool_registry[tool].performance.reliability
            for tool in selected_tools
            if tool in self.tool_registry
        ]

        base_confidence = (
            sum(reliabilities) / len(reliabilities) if reliabilities else 0.5
        )

        # Adjust based on capability coverage
        covered_capabilities = set()
        for tool in selected_tools:
            if tool in self.tool_registry:
                covered_capabilities.update(self.tool_registry[tool].capabilities)

        required_capabilities = {requirements.primary_capability}
        required_capabilities.update(requirements.secondary_capabilities)

        coverage_ratio = len(covered_capabilities & required_capabilities) / len(
            required_capabilities
        )

        return min(base_confidence * coverage_ratio, 1.0)

    def _select_fallback_tools(
        self, primary_tools: list[str], requirements: TaskRequirements
    ) -> list[str]:
        """Select fallback tools in case primary selection fails."""
        fallback_tools = []

        for tool_name, metadata in self.tool_registry.items():
            if (
                tool_name not in primary_tools
                and requirements.primary_capability in metadata.capabilities
            ):
                fallback_tools.append(tool_name)

        return fallback_tools[:2]  # Limit fallbacks

    def _assess_complexity(self, task: str) -> str:
        """Assess task complexity for optimization."""
        complexity_indicators = {
            "simple": ["quick", "basic", "simple", "fast"],
            "complex": ["comprehensive", "detailed", "thorough", "analyze", "evaluate"],
        }

        task_lower = task.lower()
        for level, indicators in complexity_indicators.items():
            if any(indicator in task_lower for indicator in indicators):
                return level

        return "moderate"

    def _calculate_success_rate(self) -> float:
        """Calculate overall success rate from execution history."""
        if not self.execution_history:
            return 1.0

        successful = sum(1 for result in self.execution_history[-50:] if result.success)
        return successful / min(len(self.execution_history), 50)

    async def _learn_from_execution(
        self, result: ExecutionResult, requirements: TaskRequirements
    ) -> dict[str, Any]:
        """Learn from execution results to improve future decisions."""
        insights = {}

        # Analyze performance vs expectations
        if result.success and result.performance_metrics:
            latency_ratio = result.performance_metrics.get("latency_vs_expected", 1.0)

            if latency_ratio > 1.5:
                insights["latency_learning"] = (
                    "Tools took longer than expected, consider faster alternatives"
                )
            elif latency_ratio < 0.7:
                insights["latency_learning"] = (
                    "Tools performed better than expected, can increase confidence"
                )

        # Track tool effectiveness patterns
        for tool in result.tools_executed:
            if tool not in self.learned_patterns:
                self.learned_patterns[tool] = {
                    "successes": 0,
                    "failures": 0,
                    "avg_latency": 0.0,
                }

            if result.success:
                self.learned_patterns[tool]["successes"] += 1
            else:
                self.learned_patterns[tool]["failures"] += 1

        return insights

    def get_performance_summary(self) -> dict[str, Any]:
        """Get comprehensive performance summary."""
        if not self.execution_history:
            return {"message": "No execution history available"}

        recent_executions = self.execution_history[-50:]

        return {
            "total_orchestrations": len(self.execution_history),
            "recent_success_rate": sum(1 for r in recent_executions if r.success)
            / len(recent_executions),
            "avg_latency_ms": sum(r.actual_latency_ms for r in recent_executions)
            / len(recent_executions),
            "tools_discovered": len(self.tool_registry),
            "learned_patterns": len(self.learned_patterns),
            "most_effective_tools": self._get_most_effective_tools(),
        }

    def _get_most_effective_tools(self) -> list[tuple[str, float]]:
        """Get most effective tools based on learned patterns."""
        effectiveness = []

        for tool, stats in self.learned_patterns.items():
            total = stats["successes"] + stats["failures"]
            if total > 0:
                success_rate = stats["successes"] / total
                effectiveness.append((tool, success_rate))

        return sorted(effectiveness, key=lambda x: x[1], reverse=True)[:5]
