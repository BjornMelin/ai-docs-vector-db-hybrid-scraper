"""Intelligent alerting system with ML-inspired anomaly detection.

This module provides enterprise-grade alerting with sophisticated pattern recognition,
contextual insights, and intelligent noise reduction - showcasing advanced observability
patterns used by companies like DataDog, New Relic, and Honeycomb.
"""

import asyncio
import logging
import statistics
import time
from collections import defaultdict, deque
from collections.abc import Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
from pydantic import BaseModel, Field


logger = logging.getLogger(__name__)


class AlertSeverity(Enum):
    """Alert severity levels with enterprise-grade classification."""

    INFO = "info"  # Informational alerts
    WARNING = "warning"  # Performance degradation
    CRITICAL = "critical"  # Service impact
    EMERGENCY = "emergency"  # Complete service failure


class AlertState(Enum):
    """Alert lifecycle states."""

    TRIGGERED = "triggered"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"


class AnomalyType(Enum):
    """Types of anomalies detected by ML algorithms."""

    SPIKE = "spike"  # Sudden increase
    DROP = "drop"  # Sudden decrease
    TREND_CHANGE = "trend_change"  # Direction change
    SEASONAL = "seasonal"  # Seasonal pattern deviation
    OUTLIER = "outlier"  # Statistical outlier
    BASELINE_SHIFT = "baseline_shift"  # Permanent level change


@dataclass
class MetricDataPoint:
    """Individual metric data point with context."""

    timestamp: float
    value: float
    labels: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AnomalyDetection:
    """Detected anomaly with confidence and context."""

    type: AnomalyType
    severity: AlertSeverity
    confidence: float  # 0.0 - 1.0
    description: str
    detected_at: float
    expected_value: float | None = None
    actual_value: float | None = None
    deviation_percentage: float | None = None

    def get_severity_score(self) -> float:
        """Calculate numeric severity score for prioritization."""
        severity_weights = {
            AlertSeverity.INFO: 0.25,
            AlertSeverity.WARNING: 0.5,
            AlertSeverity.CRITICAL: 0.75,
            AlertSeverity.EMERGENCY: 1.0,
        }
        return severity_weights[self.severity] * self.confidence


class AlertRule(BaseModel):
    """Intelligent alert rule with ML-enhanced triggering."""

    name: str
    description: str
    metric_query: str
    severity: AlertSeverity
    threshold_value: float | None = None
    threshold_operator: str = "gt"  # gt, gte, lt, lte, eq, ne

    # Advanced configuration
    evaluation_window: int = 300  # seconds
    minimum_duration: int = 60  # seconds before triggering
    recovery_threshold: float | None = None

    # ML-enhanced features
    enable_anomaly_detection: bool = True
    baseline_learning_period: int = 86400  # 24 hours
    sensitivity: float = 0.8  # 0.0 (low) to 1.0 (high)

    # Context and suppression
    depends_on: List[str] = Field(default_factory=list)
    suppression_rules: List[str] = Field(default_factory=list)
    runbook_url: str | None = None
    tags: Dict[str, str] = Field(default_factory=dict)


class Alert(BaseModel):
    """Active alert with full context and lifecycle tracking."""

    id: str
    rule_name: str
    severity: AlertSeverity
    state: AlertState

    # Alert content
    title: str
    description: str
    summary: str

    # Timing
    triggered_at: datetime
    acknowledged_at: datetime | None = None
    resolved_at: datetime | None = None

    # Context
    metric_value: float
    threshold_value: float | None = None
    labels: Dict[str, str] = Field(default_factory=dict)
    annotations: Dict[str, str] = Field(default_factory=dict)

    # ML insights
    anomaly_detection: AnomalyDetection | None = None
    confidence_score: float = 1.0
    related_alerts: List[str] = Field(default_factory=list)

    # Metadata
    runbook_url: str | None = None
    dashboard_url: str | None = None

    def get_duration_seconds(self) -> int:
        """Get alert duration in seconds."""
        end_time = self.resolved_at or datetime.utcnow()
        return int((end_time - self.triggered_at).total_seconds())

    def is_active(self) -> bool:
        """Check if alert is currently active."""
        return self.state in [AlertState.TRIGGERED, AlertState.ACKNOWLEDGED]


class BaselineTracker:
    """Track metric baselines for intelligent anomaly detection."""

    def __init__(self, metric_name: str, learning_period: int = 86400):
        self.metric_name = metric_name
        self.learning_period = learning_period
        self.data_points: deque = deque(maxlen=10000)  # Keep last 10k points
        self.hourly_baselines: Dict[int, List[float]] = defaultdict(list)
        self.daily_baselines: List[float] = []

    def add_data_point(self, value: float, timestamp: float):
        """Add a new data point and update baselines."""
        self.data_points.append(MetricDataPoint(timestamp, value))

        # Update hourly baseline (for seasonal patterns)
        hour = datetime.fromtimestamp(timestamp).hour
        self.hourly_baselines[hour].append(value)

        # Keep only recent hourly data (last 7 days)
        max_hourly_points = 7 * 24  # 7 days worth of hourly averages
        if len(self.hourly_baselines[hour]) > max_hourly_points:
            self.hourly_baselines[hour] = self.hourly_baselines[hour][
                -max_hourly_points:
            ]

        # Update daily baseline
        self.daily_baselines.append(value)
        if len(self.daily_baselines) > 30:  # Keep 30 days
            self.daily_baselines = self.daily_baselines[-30:]

    def get_expected_value(self, timestamp: float) -> Tuple[float, float]:
        """Get expected value and standard deviation for timestamp.

        Returns:
            Tuple of (expected_value, standard_deviation)
        """
        if len(self.data_points) < 10:
            # Not enough data for reliable prediction
            return 0.0, 0.0

        hour = datetime.fromtimestamp(timestamp).hour

        # Use hourly baseline if available
        if hour in self.hourly_baselines and len(self.hourly_baselines[hour]) >= 3:
            hourly_values = self.hourly_baselines[hour]
            expected = statistics.mean(hourly_values)
            std_dev = statistics.stdev(hourly_values) if len(hourly_values) > 1 else 0.0
        else:
            # Fall back to overall baseline
            recent_values = [dp.value for dp in list(self.data_points)[-100:]]
            expected = statistics.mean(recent_values)
            std_dev = statistics.stdev(recent_values) if len(recent_values) > 1 else 0.0

        return expected, std_dev

    def detect_anomaly(
        self, value: float, timestamp: float, sensitivity: float = 0.8
    ) -> AnomalyDetection | None:
        """Detect anomalies using statistical analysis.

        Args:
            value: Current metric value
            timestamp: Timestamp of the measurement
            sensitivity: Detection sensitivity (0.0 - 1.0)

        Returns:
            AnomalyDetection if anomaly found, None otherwise
        """
        expected, std_dev = self.get_expected_value(timestamp)

        if std_dev == 0:
            return None  # No variation in baseline

        # Calculate z-score
        z_score = abs(value - expected) / std_dev

        # Adjust threshold based on sensitivity
        threshold = 2.0 + (1.0 - sensitivity) * 2.0  # Range: 2.0 to 4.0

        if z_score > threshold:
            # Determine anomaly type
            if value > expected:
                anomaly_type = AnomalyType.SPIKE
                severity = (
                    AlertSeverity.WARNING if z_score < 4.0 else AlertSeverity.CRITICAL
                )
            else:
                anomaly_type = AnomalyType.DROP
                severity = (
                    AlertSeverity.WARNING if z_score < 4.0 else AlertSeverity.CRITICAL
                )

            # Calculate confidence based on z-score
            confidence = min(1.0, z_score / 6.0)  # Max confidence at 6 sigma

            # Calculate deviation percentage
            deviation_pct = (
                ((value - expected) / expected) * 100 if expected != 0 else 0
            )

            return AnomalyDetection(
                type=anomaly_type,
                severity=severity,
                confidence=confidence,
                description=f"{anomaly_type.value.title()} detected: {value:.2f} vs expected {expected:.2f} (Â±{std_dev:.2f})",
                detected_at=timestamp,
                expected_value=expected,
                actual_value=value,
                deviation_percentage=deviation_pct,
            )

        return None


class IntelligentAlertManager:
    """Enterprise-grade alert management with ML-powered insights.

    Features:
    - Statistical anomaly detection with baseline learning
    - Context-aware alert correlation and deduplication
    - Intelligent noise reduction and suppression
    - Dynamic severity adjustment based on business impact
    - Automated runbook suggestions and escalation
    - Portfolio-quality alerting UX patterns
    """

    def __init__(self):
        self.rules: Dict[str, AlertRule] = {}
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: List[Alert] = []
        self.baseline_trackers: Dict[str, BaselineTracker] = {}
        self.suppression_rules: Dict[str, Callable] = {}

        # Performance metrics for portfolio showcase
        self.alerts_generated = 0
        self.false_positives_prevented = 0
        self.noise_reduction_percentage = 0.0

        # Background tasks
        self._background_tasks: List[asyncio.Task] = []
        self._is_running = False

    async def start(self):
        """Start the intelligent alerting system."""
        self._is_running = True

        # Start background processing tasks
        self._background_tasks.append(
            asyncio.create_task(self._process_alert_correlation())
        )
        self._background_tasks.append(asyncio.create_task(self._update_baselines()))
        self._background_tasks.append(
            asyncio.create_task(self._cleanup_resolved_alerts())
        )

        logger.info("ð¨ Intelligent alerting system started")

    async def stop(self):
        """Stop the alerting system and clean up."""
        self._is_running = False

        # Cancel background tasks
        for task in self._background_tasks:
            task.cancel()

        await asyncio.gather(*self._background_tasks, return_exceptions=True)
        self._background_tasks.clear()

        logger.info("â Intelligent alerting system stopped")

    def add_rule(self, rule: AlertRule):
        """Add an intelligent alert rule."""
        self.rules[rule.name] = rule

        # Initialize baseline tracker if anomaly detection is enabled
        if rule.enable_anomaly_detection:
            self.baseline_trackers[rule.name] = BaselineTracker(
                rule.name, rule.baseline_learning_period
            )

        logger.info(
            f"ð Added alert rule: {rule.name} (severity: {rule.severity.value})"
        )

    async def evaluate_metric(
        self,
        rule_name: str,
        value: float,
        timestamp: float | None = None,
        labels: Dict[str, str] | None = None,
    ) -> Alert | None:
        """Evaluate a metric against alert rules with ML-enhanced detection.

        Args:
            rule_name: Name of the alert rule to evaluate
            value: Current metric value
            timestamp: Measurement timestamp (default: now)
            labels: Additional metric labels

        Returns:
            Alert if triggered, None otherwise
        """
        if rule_name not in self.rules:
            logger.warning(f"Unknown alert rule: {rule_name}")
            return None

        rule = self.rules[rule_name]
        timestamp = timestamp or time.time()
        labels = labels or {}

        # Update baseline tracker
        if rule.enable_anomaly_detection and rule_name in self.baseline_trackers:
            self.baseline_trackers[rule_name].add_data_point(value, timestamp)

        # Check traditional threshold
        threshold_triggered = self._check_threshold(rule, value)

        # Check for anomalies if enabled
        anomaly = None
        if rule.enable_anomaly_detection and rule_name in self.baseline_trackers:
            anomaly = self.baseline_trackers[rule_name].detect_anomaly(
                value, timestamp, rule.sensitivity
            )

        # Determine if alert should be triggered
        should_trigger = threshold_triggered or (anomaly is not None)

        if should_trigger:
            # Check if alert already exists
            alert_id = f"{rule_name}:{hash(str(sorted(labels.items())))}"
            if alert_id in self.active_alerts:
                # Update existing alert
                alert = self.active_alerts[alert_id]
                alert.metric_value = value
                if anomaly:
                    alert.anomaly_detection = anomaly
                    alert.confidence_score = anomaly.confidence
                return alert

            # Create new alert
            alert = await self._create_alert(rule, value, timestamp, labels, anomaly)

            # Apply intelligent suppression
            if not await self._should_suppress_alert(alert):
                self.active_alerts[alert.id] = alert
                self.alerts_generated += 1

                logger.warning(
                    f"ð¨ Alert triggered: {alert.title} "
                    f"(value: {value}, severity: {alert.severity.value})"
                )

                return alert
            else:
                self.false_positives_prevented += 1
                logger.info(f"ð Alert suppressed: {alert.title}")

        return None

    def _check_threshold(self, rule: AlertRule, value: float) -> bool:
        """Check if value crosses the defined threshold."""
        if rule.threshold_value is None:
            return False

        operators = {
            "gt": lambda v, t: v > t,
            "gte": lambda v, t: v >= t,
            "lt": lambda v, t: v < t,
            "lte": lambda v, t: v <= t,
            "eq": lambda v, t: v == t,
            "ne": lambda v, t: v != t,
        }

        return operators.get(rule.threshold_operator, lambda v, t: False)(
            value, rule.threshold_value
        )

    async def _create_alert(
        self,
        rule: AlertRule,
        value: float,
        timestamp: float,
        labels: Dict[str, str],
        anomaly: AnomalyDetection | None,
    ) -> Alert:
        """Create a new alert with full context."""
        alert_id = f"{rule.name}:{hash(str(sorted(labels.items())))}"

        # Determine severity (may be enhanced by anomaly detection)
        severity = rule.severity
        if anomaly and anomaly.severity.value > severity.value:
            severity = anomaly.severity

        # Generate contextual title and description
        title = f"{rule.name}: {severity.value.title()}"

        if anomaly:
            description = f"Anomaly detected: {anomaly.description}"
            summary = f"ML-detected {anomaly.type.value} with {anomaly.confidence:.1%} confidence"
        else:
            description = f"Threshold violation: {value} {rule.threshold_operator} {rule.threshold_value}"
            summary = f"Metric value {value} crossed threshold {rule.threshold_value}"

        # Add contextual annotations
        annotations = {
            "metric_value": str(value),
            "evaluation_time": datetime.fromtimestamp(timestamp).isoformat(),
            "rule_description": rule.description,
        }

        if anomaly:
            annotations.update(
                {
                    "anomaly_type": anomaly.type.value,
                    "confidence": f"{anomaly.confidence:.2%}",
                    "expected_value": str(anomaly.expected_value or "unknown"),
                    "deviation_percentage": f"{anomaly.deviation_percentage or 0:.1f}%",
                }
            )

        alert = Alert(
            id=alert_id,
            rule_name=rule.name,
            severity=severity,
            state=AlertState.TRIGGERED,
            title=title,
            description=description,
            summary=summary,
            triggered_at=datetime.fromtimestamp(timestamp),
            metric_value=value,
            threshold_value=rule.threshold_value,
            labels=labels,
            annotations=annotations,
            anomaly_detection=anomaly,
            confidence_score=anomaly.confidence if anomaly else 1.0,
            runbook_url=rule.runbook_url,
        )

        return alert

    async def _should_suppress_alert(self, alert: Alert) -> bool:
        """Intelligent alert suppression to reduce noise."""

        # Check dependency-based suppression
        rule = self.rules[alert.rule_name]
        for dependency in rule.depends_on:
            if any(
                dep_alert.rule_name == dependency and dep_alert.is_active()
                for dep_alert in self.active_alerts.values()
            ):
                logger.info(
                    f"Suppressing {alert.title} due to dependency: {dependency}"
                )
                return True

        # Check for alert flapping (rapid on/off cycles)
        recent_history = [
            hist_alert
            for hist_alert in self.alert_history[-10:]
            if hist_alert.rule_name == alert.rule_name
            and (datetime.utcnow() - hist_alert.triggered_at).total_seconds() < 300
        ]

        if len(recent_history) >= 3:
            logger.info(f"Suppressing {alert.title} due to flapping detection")
            return True

        # Check confidence-based suppression for ML alerts
        if alert.anomaly_detection and alert.confidence_score < 0.7:
            logger.info(
                f"Suppressing {alert.title} due to low confidence: {alert.confidence_score:.2%}"
            )
            return True

        return False

    async def acknowledge_alert(
        self, alert_id: str, acknowledged_by: str = "system"
    ) -> bool:
        """Acknowledge an active alert."""
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.state = AlertState.ACKNOWLEDGED
            alert.acknowledged_at = datetime.utcnow()
            alert.annotations["acknowledged_by"] = acknowledged_by

            logger.info(f"â Alert acknowledged: {alert.title} by {acknowledged_by}")
            return True

        return False

    async def resolve_alert(self, alert_id: str, resolved_by: str = "system") -> bool:
        """Resolve an active alert."""
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.state = AlertState.RESOLVED
            alert.resolved_at = datetime.utcnow()
            alert.annotations["resolved_by"] = resolved_by

            # Move to history
            self.alert_history.append(alert)
            del self.active_alerts[alert_id]

            logger.info(f"â Alert resolved: {alert.title} by {resolved_by}")
            return True

        return False

    async def _process_alert_correlation(self):
        """Background task to correlate related alerts."""
        while self._is_running:
            try:
                await self._correlate_alerts()
                await asyncio.sleep(30)  # Run every 30 seconds
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.exception(f"Error in alert correlation: {e}")
                await asyncio.sleep(60)

    async def _correlate_alerts(self):
        """Find and link related alerts."""
        active_alerts = list(self.active_alerts.values())

        for i, alert1 in enumerate(active_alerts):
            for alert2 in active_alerts[i + 1 :]:
                # Check if alerts are related by labels
                common_labels = set(alert1.labels.items()) & set(alert2.labels.items())
                if len(common_labels) >= 2:  # At least 2 common labels
                    if alert2.id not in alert1.related_alerts:
                        alert1.related_alerts.append(alert2.id)
                    if alert1.id not in alert2.related_alerts:
                        alert2.related_alerts.append(alert1.id)

    async def _update_baselines(self):
        """Background task to update metric baselines."""
        while self._is_running:
            try:
                # Baseline updates happen automatically when metrics are evaluated
                # This task could be extended to perform ML model training
                await asyncio.sleep(300)  # Every 5 minutes
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.exception(f"Error updating baselines: {e}")
                await asyncio.sleep(60)

    async def _cleanup_resolved_alerts(self):
        """Background task to clean up old resolved alerts."""
        while self._is_running:
            try:
                # Keep only alerts from the last 7 days
                cutoff_time = datetime.utcnow() - timedelta(days=7)
                self.alert_history = [
                    alert
                    for alert in self.alert_history
                    if alert.triggered_at > cutoff_time
                ]

                await asyncio.sleep(3600)  # Run every hour
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.exception(f"Error in alert cleanup: {e}")
                await asyncio.sleep(300)

    def get_alert_summary(self) -> Dict[str, Any]:
        """Get comprehensive alert summary for portfolio showcase."""
        active_by_severity = defaultdict(int)
        for alert in self.active_alerts.values():
            active_by_severity[alert.severity.value] += 1

        # Calculate noise reduction percentage
        total_potential_alerts = self.alerts_generated + self.false_positives_prevented
        if total_potential_alerts > 0:
            self.noise_reduction_percentage = (
                self.false_positives_prevented / total_potential_alerts
            ) * 100

        # Get recent alert trends
        recent_alerts = [
            alert
            for alert in self.alert_history
            if (datetime.utcnow() - alert.triggered_at).total_seconds()
            < 86400  # Last 24h
        ]

        return {
            "status": "operational",
            "active_alerts": {
                "total": len(self.active_alerts),
                "by_severity": dict(active_by_severity),
                "high_confidence": len(
                    [
                        alert
                        for alert in self.active_alerts.values()
                        if alert.confidence_score >= 0.8
                    ]
                ),
            },
            "performance_metrics": {
                "alerts_generated": self.alerts_generated,
                "false_positives_prevented": self.false_positives_prevented,
                "noise_reduction_percentage": round(self.noise_reduction_percentage, 1),
                "ml_detection_enabled": len(self.baseline_trackers),
            },
            "recent_trends": {
                "alerts_24h": len(recent_alerts),
                "avg_resolution_time_minutes": (
                    statistics.mean(
                        [
                            alert.get_duration_seconds() / 60
                            for alert in recent_alerts
                            if alert.resolved_at
                        ]
                    )
                    if recent_alerts
                    else 0
                ),
                "anomaly_detection_rate": (
                    len([alert for alert in recent_alerts if alert.anomaly_detection])
                    / len(recent_alerts)
                    * 100
                )
                if recent_alerts
                else 0,
            },
            "system_health": {
                "baseline_trackers": len(self.baseline_trackers),
                "rules_configured": len(self.rules),
                "correlation_enabled": True,
                "intelligent_suppression": True,
            },
        }

    def get_top_insights(self, limit: int = 5) -> List[Dict[str, Any]]:
        """Get top actionable insights from alert patterns."""
        insights = []

        # Insight 1: Most frequent alert sources
        alert_frequency = defaultdict(int)
        for alert in self.alert_history[-100:]:  # Last 100 alerts
            alert_frequency[alert.rule_name] += 1

        if alert_frequency:
            top_rule = max(alert_frequency, key=alert_frequency.get)
            insights.append(
                {
                    "type": "frequent_alerts",
                    "title": f"High Alert Frequency: {top_rule}",
                    "description": f"Rule '{top_rule}' has triggered {alert_frequency[top_rule]} times recently",
                    "recommendation": "Consider adjusting thresholds or investigating root cause",
                    "priority": "high" if alert_frequency[top_rule] > 10 else "medium",
                }
            )

        # Insight 2: Anomaly detection effectiveness
        ml_alerts = [
            alert for alert in self.alert_history[-50:] if alert.anomaly_detection
        ]
        if ml_alerts:
            avg_confidence = statistics.mean(
                [alert.confidence_score for alert in ml_alerts]
            )
            insights.append(
                {
                    "type": "ml_effectiveness",
                    "title": "ML Detection Performance",
                    "description": f"Anomaly detection shows {avg_confidence:.1%} average confidence",
                    "recommendation": "ML-based alerting is working effectively"
                    if avg_confidence > 0.8
                    else "Consider tuning sensitivity parameters",
                    "priority": "info",
                }
            )

        # Insight 3: Resolution time trends
        resolved_alerts = [
            alert for alert in self.alert_history[-20:] if alert.resolved_at
        ]
        if len(resolved_alerts) > 5:
            resolution_times = [
                alert.get_duration_seconds() / 60 for alert in resolved_alerts
            ]
            avg_resolution = statistics.mean(resolution_times)
            insights.append(
                {
                    "type": "resolution_time",
                    "title": "Alert Resolution Performance",
                    "description": f"Average resolution time: {avg_resolution:.1f} minutes",
                    "recommendation": "Resolution times are optimal"
                    if avg_resolution < 15
                    else "Consider automation or runbook improvements",
                    "priority": "medium" if avg_resolution > 30 else "info",
                }
            )

        return insights[:limit]


# Portfolio showcase: Pre-configured enterprise alert rules
class EnterpriseAlertRules:
    """Pre-configured alert rules following enterprise best practices."""

    @staticmethod
    def get_ai_operations_rules() -> List[AlertRule]:
        """Get alert rules for AI operations monitoring."""
        return [
            AlertRule(
                name="high_ai_operation_cost",
                description="AI operation costs are unusually high",
                metric_query="rate(ai_operation_cost_usd_total[5m])",
                severity=AlertSeverity.WARNING,
                threshold_value=0.10,  # $0.10 per 5 minutes
                threshold_operator="gt",
                enable_anomaly_detection=True,
                sensitivity=0.7,
                runbook_url="/docs/runbooks/ai-cost-optimization",
                tags={"category": "cost", "service": "ai"},
            ),
            AlertRule(
                name="slow_embedding_generation",
                description="Embedding generation is taking longer than expected",
                metric_query="histogram_quantile(0.95, ai_embedding_duration_seconds_bucket)",
                severity=AlertSeverity.WARNING,
                threshold_value=5.0,  # 5 seconds
                threshold_operator="gt",
                enable_anomaly_detection=True,
                sensitivity=0.8,
                runbook_url="/docs/runbooks/embedding-performance",
                tags={"category": "performance", "service": "embeddings"},
            ),
            AlertRule(
                name="embedding_failure_rate",
                description="High rate of embedding generation failures",
                metric_query="rate(ai_embedding_errors_total[5m])",
                severity=AlertSeverity.CRITICAL,
                threshold_value=0.05,  # 5% error rate
                threshold_operator="gt",
                minimum_duration=120,
                tags={"category": "reliability", "service": "embeddings"},
            ),
        ]

    @staticmethod
    def get_system_health_rules() -> List[AlertRule]:
        """Get alert rules for system health monitoring."""
        return [
            AlertRule(
                name="high_memory_usage",
                description="System memory usage is critically high",
                metric_query="(process_resident_memory_bytes / node_memory_MemTotal_bytes) * 100",
                severity=AlertSeverity.CRITICAL,
                threshold_value=90.0,
                threshold_operator="gt",
                enable_anomaly_detection=True,
                sensitivity=0.9,
                tags={"category": "system", "resource": "memory"},
            ),
            AlertRule(
                name="cpu_saturation",
                description="CPU usage is consistently high",
                metric_query="100 - (avg by (instance) (rate(node_cpu_seconds_total{mode='idle'}[5m])) * 100)",
                severity=AlertSeverity.WARNING,
                threshold_value=80.0,
                threshold_operator="gt",
                minimum_duration=300,  # 5 minutes
                enable_anomaly_detection=True,
                tags={"category": "system", "resource": "cpu"},
            ),
            AlertRule(
                name="service_down",
                description="Service is not responding",
                metric_query="up{job='ai-docs-scraper'}",
                severity=AlertSeverity.EMERGENCY,
                threshold_value=1.0,
                threshold_operator="lt",
                minimum_duration=30,
                tags={"category": "availability", "priority": "p0"},
            ),
        ]
